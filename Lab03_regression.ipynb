{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2024/blob/master/Lab03_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3 - Linear and Logistic Regression\n",
        "### Author: Szymon Nowakowski\n"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "------------------------\n",
        "\n",
        "In this lab, we will explore **linear regression** and **logistic regression**, two foundational techniques in **supervised learning**, a type of machine learning where models learn to map input features to specific target labels based on labeled examples.\n",
        "\n",
        "Supervised learning contrasts with **unsupervised learning**, such as clustering, where the data has no predefined labels, and the goal is to discover hidden patterns or groupings. In supervised learning, however, the goal is clear: the model learns from labeled data to predict outcomes for new, unseen data. For example:\n",
        "- In **regression** tasks, the target variable is continuous, such as predicting house prices based on square footage and location.\n",
        "- In **classification** tasks, the target variable is categorical, such as determining whether a patient has a disease based on medical test results.\n",
        "\n",
        "**Linear regression** is the simplest form of supervised learning for regression problems. It predicts a continuous target variable by fitting a straight line that best explains the relationship between input features and the target.\n",
        "\n",
        "**Logistic regression**, while sharing similarities with linear regression, is designed for classification problems. Instead of predicting a continuous value, it models the probability of a data point belonging to a particular class using a sigmoid function, which outputs values between 0 and 1.\n",
        "\n",
        "By learning from labeled examples, supervised learning mimics how humans often learn with guidance. For instance, a child might be shown pictures of animals and told which ones are dogs and cats, eventually learning to classify new animals correctly. In this lab, we will leverage similar principles to build and evaluate models for both regression and classification tasks.\n"
      ],
      "metadata": {
        "id": "39C2i5hyqrt0"
      },
      "id": "39C2i5hyqrt0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries\n",
        "-------------------\n",
        "\n",
        "Suppose $X$ represents tabular data (with the constant---intercept---term explicitly added as a column of 1s):\n",
        "\n",
        "$$\n",
        "\\begin{array}{c@{\\hspace{1em}}c}\n",
        "   & \\text{Columns = Features} \\\\\n",
        "   & \\downarrow \\\\[-6pt]\n",
        "\\text{Rows = Observations (Data Points)} \\; \\longrightarrow\n",
        "   &\n",
        "\\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
        "1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
        "\\end{bmatrix}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "and $y = (y_1, \\ldots, y_n)^T$ represents responses (a.k.a. labels, dependant variable)."
      ],
      "metadata": {
        "id": "cN6DvU4KWt1P"
      },
      "id": "cN6DvU4KWt1P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression Model and Objective\n",
        "\n",
        "In linear regression, we assume a Gaussian noise relationship between features of the $i$-th data point $x_i = X_{i\\cdot}$ and the continuous response $y_i$. Specifically, we model:\n",
        "\n",
        "$$\n",
        "y_i \\sim \\mathcal{N}(x_i^\\top \\beta, \\sigma^2),\n",
        "$$\n",
        "\n",
        "where the conditional log-likelihood of $\\beta$ given our dataset $\\{(x_i, y_i)\\}$ is proportional to the negative sum of squared errors.\n",
        "\n",
        "Since this model is fully observed—without hidden latent variables—maximizing the log-likelihood is equivalent to minimizing the sum of squared residuals:\n",
        "\n",
        "$$\n",
        "\\sum_i \\left(y_i - x_i^\\top \\beta\\right)^2.\n",
        "$$\n",
        "\n",
        "Unlike Gaussian mixtures (where latent cluster assignments and the ELBO are introduced), linear regression optimizes the likelihood directly via ordinary least squares (OLS). **Hence, no ELBO is needed.**"
      ],
      "metadata": {
        "id": "qrBK22RjaYPK"
      },
      "id": "qrBK22RjaYPK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### $\\hat \\beta$ Estimation\n",
        "\n",
        "Assuming the standard linear regression model:\n",
        "\n",
        "$$ y = X \\beta + \\varepsilon $$\n",
        "\n",
        "where $\\varepsilon$ is the error term with expectation $E(\\varepsilon) = 0$ and variance $\\text{Var}(\\varepsilon) = \\sigma^2 I_n$, the least squares estimator is:\n",
        "\n",
        "$$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$\n",
        "\n",
        "provided that $X^T X$ is invertible."
      ],
      "metadata": {
        "id": "jeebUF9_aaSu"
      },
      "id": "jeebUF9_aaSu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Properties of the Least Squares Estimator $\\hat{\\beta}$\n",
        "\n",
        "1. **Unbiasedness of $\\hat{\\beta}$**\n",
        "\n",
        "  Taking expectation of $ \\hat{\\beta} = (X^T X)^{-1} X^T y $:\n",
        "\n",
        "  $$ E(\\hat{\\beta}) = E\\left[ (X^T X)^{-1} X^T y \\right] $$\n",
        "\n",
        "  Substituting $y = X \\beta + \\varepsilon$:\n",
        "\n",
        "  $$ E(\\hat{\\beta}) = (X^T X)^{-1} X^T E(y) = (X^T X)^{-1} X^T X \\beta = \\beta $$\n",
        "\n",
        "  $$ E(\\hat{\\beta}) = \\beta $$\n",
        "  \n",
        "  Thus, $\\hat{\\beta}$ is **unbiased**.\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "2. **Variance of $\\hat{\\beta}$**\n",
        "\n",
        "  The variance of $\\hat{\\beta}$ is:\n",
        "\n",
        "  $$ \\text{Var}(\\hat{\\beta}) = E \\left[ (\\hat{\\beta} - \\beta)(\\hat{\\beta} - \\beta)^T \\right] $$\n",
        "\n",
        "  Expanding using $y = X \\beta + \\varepsilon$:\n",
        "\n",
        "  $$ \\text{Var}(\\hat{\\beta}) = (X^T X)^{-1} X^T \\text{Var}(y) X (X^T X)^{-1} $$\n",
        "\n",
        "  Since $\\text{Var}(y) = \\sigma^2 I_n$, we get:\n",
        "\n",
        "  $$ \\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1} $$\n",
        "\n",
        "\n",
        "3. **Estimator of the Variance $\\sigma^2$**\n",
        "\n",
        "  The residual sum of squares (RSS) is:\n",
        "\n",
        "  $$\n",
        "  RSS = (y - X\\hat{\\beta})^T (y - X\\hat{\\beta})\n",
        "  $$\n",
        "\n",
        "  The unbiased estimator of $\\sigma^2$ is:\n",
        "\n",
        "  $$\n",
        "  \\hat{\\sigma}^2 = \\frac{RSS}{n - (p+1)} = \\frac{(y - X\\hat{\\beta})^T (y - X\\hat{\\beta})}{n - (p+1)}\n",
        "  $$\n",
        "\n",
        "  where $p+1$ is the number of parameters (including the intercept).\n",
        "\n"
      ],
      "metadata": {
        "id": "5ER_FjRCdA9G"
      },
      "id": "5ER_FjRCdA9G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Hat Matrix $H$\n",
        "\n",
        "The fitted values $\\hat{y}$ are given by:\n",
        "\n",
        "$$\\hat{y} = X \\hat{\\beta} = X (X^T X)^{-1} X^T y$$\n",
        "\n",
        "Thus, the hat matrix $H$ is:\n",
        "\n",
        "$$H = X (X^T X)^{-1} X^T$$\n",
        "\n",
        "which is an $n \\times n$ matrix that projects $y$ onto the column space of $X$, effectively mapping the observed responses $y$ to the predicted responses $\\hat{y}$."
      ],
      "metadata": {
        "id": "N7yB87YZachz"
      },
      "id": "N7yB87YZachz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Properties of $H$\n",
        "\n",
        "\n",
        "- **Idempotent**: $$H^2 = H$$\n",
        "  meaning applying it twice has the same effect as applying it once (so $H$ is a projection)\n",
        "- **Symmetric**: $$H^T = H$$ (hence, orthogonal projection)\n",
        "- For $v \\in \\text{Col}(X)$ we have $Hv = v$\n",
        "\n",
        "Thus $H$ is an **orthogonal projection** onto $\\text{Col}(X)$.\n",
        "\n",
        "- **Trace**: The trace of $H$ is equal to the number of parameters, $p+1$, i.e.,\n",
        "  \n",
        "  $$\\text{tr}(H) = p+1$$\n",
        "  \n",
        "  which represents the effective number of parameters in the regression model.\n",
        "\n",
        "This matrix plays a key role in diagnostics, such as leverage and residual analysis in linear regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "nSjj00kaiM9r"
      },
      "id": "nSjj00kaiM9r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Logistic Regression Model and Objective\n",
        "\n",
        "In logistic regression, each binary label $y_i \\in \\{0, 1\\}$ is modeled as being generated from a Bernoulli distribution. The Bernoulli parameter is the logistic sigmoid function applied to a linear combination of the features:\n",
        "\n",
        "$$\n",
        "y_i \\sim \\text{Bernoulli}\\left(\\sigma(x_i^\\top \\beta)\\right),\n",
        "$$\n",
        "\n",
        "where the logistic sigmoid function is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
        "$$\n",
        "\n",
        "This results in a log-likelihood corresponding to the logistic (cross-entropy) loss, which remains fully tractable as no latent variables are introduced. Each $y_i$ is observed, allowing us to optimize the negative log-likelihood:\n",
        "\n",
        "$$\n",
        "-\\sum_{i=1}^N \\left[ y_i \\log(\\sigma(x_i^\\top \\beta)) + (1 - y_i) \\log(1 - \\sigma(x_i^\\top \\beta)) \\right].\n",
        "$$\n",
        "\n",
        "This optimization can be performed directly via gradient ascent or equivalent methods. We will see how Iterated Reweighted Least Squares ( asecond-order method) can iteratively be used to optimize the objective.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nR8DGpP_hlFO"
      },
      "id": "nR8DGpP_hlFO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "-------------------"
      ],
      "metadata": {
        "id": "JdYhKZt2hzuD"
      },
      "id": "JdYhKZt2hzuD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working example - computer processor performance\n",
        "\n",
        "We will use `cpus` dataset which comes from the `MASS` R package. Therefore we need to download this R package into Python. It is perfectly possible."
      ],
      "metadata": {
        "id": "XhQq-Grsh0iv"
      },
      "id": "XhQq-Grsh0iv"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rpy2.robjects import r, pandas2ri\n",
        "\n",
        "# Activate the pandas-R interface\n",
        "pandas2ri.activate()\n",
        "\n",
        "# Load the cpus dataset from the MASS package in R\n",
        "r('install.packages(\"MASS\", repos=\"http://cran.us.r-project.org\")')\n",
        "r('library(MASS)')\n",
        "cpus_df = pd.DataFrame(r('as.data.frame(cpus)'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "04KHNwg_aArv",
        "outputId": "a4ca422d-44cd-41e3-f8bf-c49b71d89e36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "04KHNwg_aArv",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: trying URL 'http://cran.us.r-project.org/src/contrib/MASS_7.3-65.tar.gz'\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Content type 'application/x-gzip'\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]:  length 510322 bytes (498 KB)\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: downloaded 498 KB\n",
            "\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The downloaded source packages are in\n",
            "\t‘/tmp/RtmpYY4ajj/downloaded_packages’\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dataset\n",
        "print(\"CPU Dataset Loaded from R:\")\n",
        "print(cpus_df.head())\n",
        "\n",
        "# Drop the first and last columns (processor name and estimated performance)\n",
        "cpus_df = cpus_df.iloc[:, 1:-1]"
      ],
      "metadata": {
        "id": "QmWNLMY1b_Jd",
        "outputId": "8c4c9b33-6a8c-4fee-92d3-a14506444eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QmWNLMY1b_Jd",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Dataset Loaded from R:\n",
            "    0      1      2      3      4      5      6      7      8      9    ...  \\\n",
            "0     1      3      2      4      5      6      8      9     10      7  ...   \n",
            "1   125     29     29     29     29     26     23     23     23     23  ...   \n",
            "2   256   8000   8000   8000   8000   8000  16000  16000  16000  32000  ...   \n",
            "3  6000  32000  32000  32000  16000  32000  32000  32000  64000  64000  ...   \n",
            "4   256     32     32     32     32     64     64     64     64    128  ...   \n",
            "\n",
            "     199   200   201   202   203   204   205   206   207   208  \n",
            "0    200   201   202   203   204   205   206   207   209   208  \n",
            "1     30   180   180   180   180   124    98   125   480   480  \n",
            "2   8000   262   512   262   512  1000  1000  2000   512  1000  \n",
            "3  64000  4000  4000  4000  4000  8000  8000  8000  8000  4000  \n",
            "4    128     0     0     0     0     0    32     0    32     0  \n",
            "\n",
            "[5 rows x 209 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data overview\n",
        "print(\"\\nDataset summary:\")\n",
        "print(cpus_df.describe())"
      ],
      "metadata": {
        "id": "JHKguAxpdIYF",
        "outputId": "5fe609ca-6017-41bd-9efe-e78f4c792e3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JHKguAxpdIYF",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset summary:\n",
            "                1             2             3             4             5    \\\n",
            "count      9.000000      9.000000      9.000000      9.000000      9.000000   \n",
            "mean    4514.000000   4508.444444   4503.333333   2706.000000   4527.111111   \n",
            "std    10634.575497  10637.085622  10639.410592   5636.212536  10628.608240   \n",
            "min        3.000000      2.000000      4.000000      5.000000      6.000000   \n",
            "25%       29.000000     29.000000     29.000000     16.000000     26.000000   \n",
            "50%       32.000000     32.000000     32.000000     32.000000     64.000000   \n",
            "75%      269.000000    253.000000    253.000000    132.000000    318.000000   \n",
            "max    32000.000000  32000.000000  32000.000000  16000.000000  32000.000000   \n",
            "\n",
            "                6             7             8             9            10   \\\n",
            "count      9.000000      9.000000      9.000000      9.000000     9.000000   \n",
            "mean    5432.333333   5446.000000   9058.888889  10959.555556   497.222222   \n",
            "std    11262.180129  11255.332203  21255.588356  22477.368452   996.056572   \n",
            "min        8.000000      9.000000     10.000000      7.000000     0.000000   \n",
            "25%       23.000000     23.000000     23.000000     32.000000     2.000000   \n",
            "50%       64.000000     64.000000     64.000000    128.000000    23.000000   \n",
            "75%      381.000000    489.000000    749.000000   1238.000000   400.000000   \n",
            "max    32000.000000  32000.000000  64000.000000  64000.000000  3000.000000   \n",
            "\n",
            "       ...           198           199          200          201          202  \\\n",
            "count  ...      9.000000      9.000000     9.000000     9.000000     9.000000   \n",
            "mean   ...   8260.777778   8297.111111   520.333333   548.444444   521.222222   \n",
            "std    ...  21058.248317  21043.968592  1308.885690  1305.230259  1308.534942   \n",
            "min    ...     12.000000     12.000000     0.000000     0.000000     0.000000   \n",
            "25%    ...     96.000000    128.000000     3.000000     3.000000     3.000000   \n",
            "50%    ...    199.000000    200.000000    24.000000    24.000000    24.000000   \n",
            "75%    ...    919.000000   1150.000000   201.000000   202.000000   203.000000   \n",
            "max    ...  64000.000000  64000.000000  4000.000000  4000.000000  4000.000000   \n",
            "\n",
            "               203          204          205          206          207  \n",
            "count     9.000000     9.000000     9.000000     9.000000     9.000000  \n",
            "mean    549.444444  1046.333333  1049.111111  1160.111111  1038.555556  \n",
            "std    1304.807659  2626.958270  2625.643752  2644.577917  2618.123570  \n",
            "min       0.000000     0.000000     2.000000     0.000000     0.000000  \n",
            "25%       3.000000     8.000000    32.000000    14.000000    32.000000  \n",
            "50%      24.000000    42.000000    50.000000    52.000000    67.000000  \n",
            "75%     204.000000   205.000000   206.000000   207.000000   480.000000  \n",
            "max    4000.000000  8000.000000  8000.000000  8000.000000  8000.000000  \n",
            "\n",
            "[8 rows x 207 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression with `sklearn`\n",
        "\n",
        "`sklearn.linear_model.LinearRegression` does not include built-in diagnostic plots. The `LinearRegression` class in `scikit-learn` is focused solely on fitting models and making predictions, but it does not provide tools for visualization or diagnostic analysis.\n",
        "\n",
        "However, you can create diagnostic plots **manually** using libraries like `matplotlib`, `seaborn`, or **automatically** with `statsmodels`. We shall show the mixture of those technics in a moment.\n",
        "\n",
        "First, let's estimate and test a new linear regression model using `sklearn`."
      ],
      "metadata": {
        "id": "MPFWf7BrvNot"
      },
      "id": "MPFWf7BrvNot"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Split dataset into predictors and response variable\n",
        "X = cpus_df.iloc[:, :-1]  # All columns except the last one\n",
        "y = cpus_df.iloc[:, -1]   # The last column is the response variable\n",
        "\n",
        "# Split into training and test sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Fit a linear regression model\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Display coefficients\n",
        "print(\"\\nModel coefficients:\")\n",
        "for feature, coef in zip(X.columns, model.coef_):\n",
        "    print(f\"{feature}: {coef}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nMean Squared Error (MSE): {mse}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R² Score: {r2}\")"
      ],
      "metadata": {
        "id": "3NB7yJHodUIc"
      },
      "id": "3NB7yJHodUIc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $R^2$ Score Refresher\n",
        "\n",
        "The **$R^2$ score** (coefficient of determination) measures how well a regression model explains the variability of the response variable. It is defined as:\n",
        "\n",
        "$$ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $$\n",
        "\n",
        "where the numerator represents the **residual sum of squares (RSS)** and the denominator is the **total sum of squares (TSS)**. An $R^2$ value of 1 indicates a perfect fit, while 0 means the model explains no variance beyond the mean. Negative values imply the model performs worse than a simple mean prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "UAB4Boj5iX1d"
      },
      "id": "UAB4Boj5iX1d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diagnostic Plots\n"
      ],
      "metadata": {
        "id": "2sSf9A4mflyO"
      },
      "id": "2sSf9A4mflyO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diagnostic Plot Differences: R vs Python\n",
        "\n",
        "The difference in behavior arises from how **R** and **`statsmodels` (Python)** handle some diagnostic plotting for regression models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "12JvqZSTfu2m"
      },
      "id": "12JvqZSTfu2m"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add a constant column for intercept (required for statsmodels)\n",
        "X_train = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the model using statsmodels\n",
        "model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "# Summary of the model\n",
        "print(model.summary())\n",
        "\n",
        "sm.graphics.plot_fit(model\n",
        "\n",
        "# Diagnostic plots\n",
        "# 1. Residuals vs Fitted\n",
        "for predictor in X_train.columns[1:]:  # Skip the constant column\n",
        "    sm.graphics.plot_fit(model, exog_idx=predictor)\n",
        "    plt.xlabel(predictor)\n",
        "    plt.ylabel(\"Performance\")\n",
        "    plt.title(f\"Fitted vs Residuals for {predictor}\")\n",
        "    plt.show()\n",
        "\n",
        "# 2. Q-Q plot for residuals\n",
        "sm.graphics.qqplot(model.resid, line='45', fit=True)\n",
        "plt.title(\"Q-Q Plot\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Residuals vs Leverage\n",
        "sm.graphics.plot_leverage_resid2(model)\n",
        "plt.title(\"Residuals vs Leverage\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VYVQSURsdrJJ"
      },
      "id": "VYVQSURsdrJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residuals vs Leverage\n",
        "\n",
        "The above plot is quite different the the R plot:\n",
        "\n",
        "<img src=\"https://github.com/SzymonNowakowski/Machine-Learning-2024/blob/master/residuals_vs_leverage_in_R.PNG?raw=1\" alt=\"residuals vs levarage plot in R\" width=\"600\" height=\"400\">\n",
        "\n",
        "Note that\n",
        "\n",
        "- the axes are reversed,\n",
        "- the standardized residuals are squared in Python and they are not R,\n",
        "- the Cook's boundaries are included for easier interpretation. You would have to add that manually in Python"
      ],
      "metadata": {
        "id": "WdmEycJ9ldYS"
      },
      "id": "WdmEycJ9ldYS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale-Location needs to be simulated, too\n",
        "\n",
        "<img src=\"https://github.com/SzymonNowakowski/Machine-Learning-2024/blob/master/scale_vs_location_in_R.PNG?raw=1\" alt=\"scale vs location plot in R\" width=\"600\" height=\"400\">"
      ],
      "metadata": {
        "id": "usZWOvsjhEUB"
      },
      "id": "usZWOvsjhEUB"
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "# 4. Scale-Location Plot\n",
        "residuals = model.resid\n",
        "fitted = model.fittedvalues\n",
        "sqrt_std_residuals = np.sqrt(np.abs(residuals))\n",
        "\n",
        "# Lowess smoothing for trend line\n",
        "lowess_smooth = lowess(sqrt_std_residuals, fitted, frac=0.3)  # frac controls the smoothing\n",
        "\n",
        "plt.scatter(fitted, sqrt_std_residuals, alpha=0.7)\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n",
        "plt.plot(lowess_smooth[:, 0], lowess_smooth[:, 1], color=\"blue\", linewidth=2, label=\"Trend Line (Lowess)\")\n",
        "\n",
        "plt.title(\"Scale-Location Plot\")\n",
        "plt.xlabel(\"Fitted values\")\n",
        "plt.ylabel(\"Sqrt(Standardized Residuals)\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "dURkRM9ohDiF"
      },
      "id": "dURkRM9ohDiF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residuals vs Fitted\n",
        "In **R**, diagnostic plot **\"Residuals vs Fitted\"** is a model-level diagnostics. It relies on the fitted values ($\\hat{y}$) and residuals ($y - \\hat{y}$), which are properties of the regression model itself. The plot does not depend on specific predictors.\n",
        "\n",
        "<img src=\"https://github.com/SzymonNowakowski/Machine-Learning-2024/blob/master/residuals_vs_fitted_in_R.PNG?raw=1\" alt=\"residuals vs fitted plot in R\" width=\"600\" height=\"400\">"
      ],
      "metadata": {
        "id": "2-d9XRjGfxUc"
      },
      "id": "2-d9XRjGfxUc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Python's `sm.graphics.plot_fit`:\n",
        "In **Python**, the `sm.graphics.plot_fit` function in **statsmodels** is designed specifically to visualize **one predictor variable at a time**. It shows the relationship between the chosen predictor and the fitted values, making it more focused than R's general diagnostic plot."
      ],
      "metadata": {
        "id": "AzjvMT8OfzZ6"
      },
      "id": "AzjvMT8OfzZ6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How to Get an R-Like \"Residuals vs Fitted\" Plot in Python\n",
        "\n",
        "We can emulate R's behavior and plot Residuals vs Fitted independently of specific predictors with a custom plot."
      ],
      "metadata": {
        "id": "9kfvTOWvgnME"
      },
      "id": "9kfvTOWvgnME"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Residuals vs Fitted Plot\n",
        "fitted_values = model.fittedvalues  # Predicted values (fitted values)\n",
        "residuals = model.resid  # Residuals\n",
        "\n",
        "# Lowess smoothing for trend line\n",
        "lowess_smooth = lowess(residuals, fitted_values, frac=0.3)  # frac controls the smoothing\n",
        "\n",
        "plt.scatter(fitted_values, residuals, alpha=0.7)\n",
        "plt.axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=1)  # Horizontal line at 0\n",
        "plt.plot(lowess_smooth[:, 0], lowess_smooth[:, 1], color=\"blue\", linewidth=2, label=\"Trend Line (Lowess)\")\n",
        "\n",
        "plt.title(\"Residuals vs Fitted\")\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1ZC4IDEhgr-w"
      },
      "id": "1ZC4IDEhgr-w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What to Look for in the Charts\n",
        "\n",
        "The diagnostic plots are essential for diagnosing issues like non-linearity, heteroscedasticity, or influential observations."
      ],
      "metadata": {
        "id": "7BKyGhg8bZfZ"
      },
      "id": "7BKyGhg8bZfZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ideal (Correct) Behavior\n",
        "1. **Random Scatter Around Zero**:\n",
        "   - Residuals should be randomly distributed around the horizontal line $y = 0$.\n",
        "   - This indicates that the model is capturing all patterns in the data, leaving only random noise in the residuals.\n",
        "\n",
        "2. **No Clear Patterns**:\n",
        "   - Residuals should not show a trend, curve, or other systematic structure.\n",
        "   - A lack of patterns suggests that the model adequately describes the relationship between predictors and the target variable.\n",
        "\n",
        "3. **Homoscedasticity (Constant Variance)**:\n",
        "   - The spread of residuals should remain constant across all fitted values.\n",
        "   - Residuals should not fan out or converge, indicating that variance is independent of the fitted values."
      ],
      "metadata": {
        "id": "GG19H68YbfMx"
      },
      "id": "GG19H68YbfMx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Issues (Incorrect Behavior)\n",
        "\n",
        "1. **Non-Linearity**:\n",
        "   - **Behavior**: Residuals show a curved pattern.\n",
        "   - **Interpretation**: The relationship between predictors and the target is non-linear, and the linear model is insufficient.\n",
        "   - **Fix**: Consider transforming predictors (e.g., polynomial terms) or using a non-linear model.\n",
        "\n",
        "2. **Heteroscedasticity (Non-Constant Variance)**:\n",
        "   - **Behavior**: Residuals fan out (increasing spread) or converge (decreasing spread).\n",
        "   - **Interpretation**: The variance of residuals depends on the fitted values.\n",
        "   - **Fix**: Use weighted least squares (WLS) or transform the dependent variable.\n",
        "\n",
        "3. **Outliers or Influential Points**:\n",
        "   - **Behavior**: Individual points far away from the horizontal line $y = 0$ or with high leverage.\n",
        "   - **Interpretation**: Certain observations have undue influence on the regression model.\n",
        "   - **Fix**: Investigate and potentially remove or transform these points.\n",
        "\n",
        "4. **Systematic Pattern**:\n",
        "   - **Behavior**: A pattern in residuals, such as waves or clustering.\n",
        "   - **Interpretation**: The model is missing important features or interactions.\n",
        "   - **Fix**: Add missing predictors or interaction terms.\n"
      ],
      "metadata": {
        "id": "hvzrRl5IbiX_"
      },
      "id": "hvzrRl5IbiX_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Plots\n",
        "\n",
        "1. Correct Behavior\n",
        "- **Description**: Residuals are randomly scattered around zero, with no discernible patterns or variance issues.\n",
        "- **What It Means**: The model fits the data well and captures the underlying relationship.\n",
        "\n",
        "```\n",
        "Residuals\n",
        "|\n",
        "|    .  .         .   .  \n",
        "|   .        .    .        \n",
        "|       .     .       .    \n",
        "| .   .   .       .         \n",
        "|_______________________ Fitted\n",
        "```\n",
        "\n",
        "2. Non-Linearity\n",
        "- **Description**: A curved pattern in the residuals.\n",
        "- **What It Means**: The relationship is non-linear, and the model is insufficient.\n",
        "\n",
        "```\n",
        "Residuals\n",
        "|\n",
        "|    .  .          \n",
        "|       .   .     \n",
        "|          .   \n",
        "|   .    .   .    \n",
        "|_______________________ Fitted\n",
        "         (curve)\n",
        "```\n",
        "\n",
        "3. Heteroscedasticity\n",
        "- **Description**: Residuals fan out or narrow as fitted values increase.\n",
        "- **What It Means**: Variance is not constant, violating an assumption of linear regression.\n",
        "\n",
        "```\n",
        "Residuals\n",
        "|\n",
        "| .   .  \n",
        "|  .     .  \n",
        "|   .       .    \n",
        "|    .          .    \n",
        "|_______________________ Fitted\n",
        "```\n",
        "\n",
        "4. Outliers or Influential Points\n",
        "- **Description**: Points far away from $y = 0$ or with extreme fitted values.\n",
        "- **What It Means**: Certain data points have a disproportionate influence on the model.\n",
        "\n",
        "```\n",
        "Residuals\n",
        "|\n",
        "|  .  .            \n",
        "|   .              \n",
        "|         .      (outlier)\n",
        "|       .          .       \n",
        "|_______________________ Fitted\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y66KxIFmxjKZ"
      },
      "id": "Y66KxIFmxjKZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Fix Issues\n",
        "\n",
        "1. **Non-Linearity**:\n",
        "   - Add polynomial terms (e.g., $x^2$) or interactions to the model.\n",
        "   - Use non-linear regression techniques.\n",
        "\n",
        "2. **Heteroscedasticity**:\n",
        "   - Transform the dependent variable (e.g., log or square root).\n",
        "   - Use weighted least squares (WLS).\n",
        "\n",
        "3. **Outliers or Influential Points**:\n",
        "   - Use diagnostic tools like **Cook's Distance** to identify influential points.\n",
        "   - Remove or adjust these points if they are errors or anomalies.\n",
        "\n",
        "4. **Systematic Patterns**:\n",
        "   - Add missing predictors, interactions, or non-linear transformations.\n"
      ],
      "metadata": {
        "id": "4BrRDIb6bPF4"
      },
      "id": "4BrRDIb6bPF4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nonlinear Transformations with Linear Regression Models\n",
        "\n",
        "In a James Bond movie, there’s a memorable scene where a young woman, water skiing, passes by James Bond as he relaxes on a terrace with a martini glass. She splashes him with water and exclaims, \"Oh my God, I made you all wet!\" To which Bond coolly replies, \"Yes, but my martini is still dry.\"\n",
        "\n",
        "This is because the terms *wet* and *dry* refer to somewhat different contexts.\n",
        "\n",
        "Similarly, in regression, even when we apply nonlinear transformations to variables, the model remains linear. This is because the term *linear* refers specifically to the relationship between the response variable and the predictors (transformed or not).\n",
        "\n",
        "As a result of analyzing the diagnostic charts, we conclude that the response variable appears to increase exponentially. Additional support for this observation comes from [Moore's Law](https://pl.wikipedia.org/wiki/Prawo_Moore%E2%80%99a), which states that various parameters of computer systems (including processor performance) double every 24 months—indicating exponential growth. Therefore, we will attempt to use a (nonlinear) logarithmic transformation for the response variable, `perf`, to bring it to a linear scale.\n",
        "\n",
        "Formally, we will now apply a linear regression model to the nonlinearly transformed variable `log(perf)`.\n",
        "\n",
        "**Note:** Other nonlinear transformations can be applied to both the response variable and the predictors, particularly if they are justified by the phenomenon being modeled. For example, if we are modeling the `volume` of a cylinder, we might conclude that the `volume` is a linear function not of the base `radius`, but of the square of the base `radius`. In this case, we would use a (nonlinear) square function for the predictor `radius`.\n",
        "\n",
        "We have two options to achieve this:\n",
        "\n",
        "1. Apply a logarith function to the column in a dataframe and *after that* proceed excactly as before (create a model, create diagnostic charts)\n",
        "\n",
        "2. Using the *Formula API* in `statsmodels`: Similar to R, `statsmodels` provides a formula interface that supports transformations directly within the regression formula. You do not need to manually transform the data first. It is usefull for quick modelling.\n",
        "\n",
        "We will proceed the second way.\n"
      ],
      "metadata": {
        "id": "9Hcfwn3-1I-N"
      },
      "id": "9Hcfwn3-1I-N"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(cpus_df, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Fit the model using statsmodels formula API with a log transformation on 'perf'\n",
        "model = sm.OLS.from_formula('np.log(perf) ~ chmax + chmin + cach + mmax + mmin + syct + 1', data=train_df).fit()\n",
        "# Note, that adding a constant column for intercept is not needed now, we will add that explicitly in a formula\n",
        "\n",
        "# Summary of the model\n",
        "print(model.summary())\n",
        "\n",
        "# Diagnostic plots\n",
        "# 1. Residuals vs Fitted\n",
        "for predictor in train_df.columns[1:-1]:  # Iterate over predictors, skip the constant and perf itself\n",
        "    sm.graphics.plot_fit(model, exog_idx=predictor)\n",
        "    plt.xlabel(predictor)\n",
        "    plt.ylabel(\"Log Performance\")\n",
        "    plt.title(f\"Fitted vs Residuals for {predictor}\")\n",
        "    plt.show()\n",
        "\n",
        "# 2. Q-Q plot for residuals\n",
        "sm.graphics.qqplot(model.resid, line='45', fit=True)\n",
        "plt.title(\"Q-Q Plot\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Residuals vs Leverage\n",
        "sm.graphics.plot_leverage_resid2(model)\n",
        "plt.title(\"Residuals vs Leverage\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BAPZuKiI5XMd"
      },
      "id": "BAPZuKiI5XMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Box-Cox Transform\n",
        "\n",
        "Another popular transformation is the Box-Cox transformation, where for a certain parameter $\\lambda$, we apply the transformation:\n",
        "$$\n",
        "Y_i \\rightarrow \\frac{Y_i^\\lambda - 1}{\\lambda}\n",
        "$$\n",
        "for the response. It can be used on the predictor, too\n",
        "\n",
        "For $\\lambda = 0$ a limit $\\lambda \\rightarrow 0$ is used which is a logarithmic transformation. The parameter $\\lambda$ is chosen to make the distribution of residuals as close as possible to a normal distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "ek4VWOLYItRk"
      },
      "id": "ek4VWOLYItRk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Interpreting $\\lambda$**\n",
        "\n",
        "The $\\lambda$ value determines the nature of the transformation:\n",
        "\n",
        "- $\\lambda = 1$: No transformation (original data is used).\n",
        "- $\\lambda = 0$: Log transformation is applied ($\\log(y)$).\n",
        "- $-1 < \\lambda < 1$: Partial transformation for skewness reduction.\n",
        "- $\\lambda < 0$: Reciprocal transformation, which is rarely used but may apply in some cases."
      ],
      "metadata": {
        "id": "gV8Hg5bPJz2Y"
      },
      "id": "gV8Hg5bPJz2Y"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import boxcox\n",
        "\n",
        "# Apply Box-Cox transformation to predictors\n",
        "lambdas = {}  # Store lambda values for each predictor\n",
        "for column in ['cach', 'chmin', 'chmax', 'mmax', 'mmin', 'syct']:\n",
        "    print(f\"Finding the Box-Cox transform for {column}\")\n",
        "    train_df[column + '_bc'], lambdas[column] = boxcox(train_df[column]+(1 if column in ['cach', 'chmin', 'chmax'] else 0))\n",
        "    # Output the lambda value\n",
        "    print(f\"Optimal λ (lambda) used for the Box-Cox transformation of {column}: {lambdas[column]}\")\n",
        "\n",
        "# Fit the model with the transformed predictors\n",
        "formula = 'np.log(perf) ~ chmax_bc + chmin_bc + cach_bc + mmax_bc + mmin_bc + syct_bc + 1'\n",
        "# Note, that adding a constant column for intercept is not needed now, we will add that explicitly in a formula\n",
        "boxcox_model = sm.OLS.from_formula(formula, data=train_df).fit()\n",
        "\n",
        "# Summary of the model\n",
        "print(boxcox_model.summary())\n",
        "\n",
        "# Diagnostic plots\n",
        "# Residuals vs Fitted\n",
        "for column in ['chmax_bc', 'chmin_bc', 'cach_bc', 'mmax_bc', 'mmin_bc', 'syct_bc']:  # Iterate over transformed predictors\n",
        "    sm.graphics.plot_fit(boxcox_model, exog_idx=column)\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Log Performance\")\n",
        "    plt.title(f\"Fitted vs Residuals for {column}\")\n",
        "    plt.show()\n",
        "\n",
        "# Q-Q plot for residuals\n",
        "sm.graphics.qqplot(boxcox_model.resid, line='45', fit=True)\n",
        "plt.title(\"Q-Q Plot\")\n",
        "plt.show()\n",
        "\n",
        "# Residuals vs Leverage\n",
        "sm.graphics.plot_leverage_resid2(boxcox_model)\n",
        "plt.title(\"Residuals vs Leverage\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d4YewwmyHSFt"
      },
      "id": "d4YewwmyHSFt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Predictors from a Model\n",
        "\n",
        "Since each coefficient $\\hat{\\beta}_i$ is estimated, but the true (unknown) value of the coefficient is $\\beta_i$, we can ask whether this true value (although unknown) might actually be zero. In other words, does the given predictor have **any influence on the response variable**?\n",
        "\n",
        "Recall, the model equation can be stated as:\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon\n",
        "$$\n",
        "where $\\epsilon$ represents the error term and $\\beta_i=0$ represent predictors with **no influence on the response**.\n",
        "\n",
        "This question leads us to the problem of testing the null hypothesis:\n",
        "$$\n",
        "H_0: \\beta_i = 0\n",
        "$$\n",
        "under the alternative (two-sided) hypothesis:\n",
        "$$\n",
        "H_1: \\beta_i \\neq 0.\n",
        "$$\n",
        "\n",
        "Since the distribution of the estimator $\\hat{\\beta}_i$ is known (it follows a Student's t-distribution with a mean of $\\beta_i$), `statsmodels` automatically calculates the p-value for testing these hypotheses. The column `Pr(>|t|)` in the model summary contains these values.\n",
        "\n",
        "This leads us to dropping the `chmin_bc` and `syct_bc` variables."
      ],
      "metadata": {
        "id": "QuVJ63cfPC9p"
      },
      "id": "QuVJ63cfPC9p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model with the transformed predictors\n",
        "formula = 'np.log(perf) ~ chmax_bc + cach_bc + mmax_bc + mmin_bc + 1'\n",
        "# Note, that adding a constant column for intercept is not needed now, we will add that explicitly in a formula\n",
        "boxcox_model = sm.OLS.from_formula(formula, data=train_df).fit()\n",
        "\n",
        "# Summary of the model\n",
        "print(boxcox_model.summary())\n",
        "\n",
        "# Diagnostic plots\n",
        "# Residuals vs Fitted\n",
        "for column in ['chmax_bc', 'cach_bc', 'mmax_bc', 'mmin_bc']:  # Iterate over transformed predictors\n",
        "    sm.graphics.plot_fit(boxcox_model, exog_idx=column)\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Log Performance\")\n",
        "    plt.title(f\"Fitted vs Residuals for {column}\")\n",
        "    plt.show()\n",
        "\n",
        "# Q-Q plot for residuals\n",
        "sm.graphics.qqplot(boxcox_model.resid, line='45', fit=True)\n",
        "plt.title(\"Q-Q Plot\")\n",
        "plt.show()\n",
        "\n",
        "# Residuals vs Leverage\n",
        "sm.graphics.plot_leverage_resid2(boxcox_model)\n",
        "plt.title(\"Residuals vs Leverage\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2k8o0UmQwc0"
      },
      "id": "B2k8o0UmQwc0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "--------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "vsfg8JJLbxwa"
      },
      "id": "vsfg8JJLbxwa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Logistic Regression with Linear Regression\n",
        "\n",
        "**Objective:** Highlight why logistic regression is better suited for classification tasks.\n",
        "\n",
        "**Experiment:**\n",
        "Use the same dataset to fit both linear regression and logistic regression. **Beware** that the logistic regression is by default penalized in `scikit-learn`.\n",
        "\n",
        "We will show how linear regression produces inappropriate probabilities (e.g., negative or >1) and plot predictions for both methods to compare."
      ],
      "metadata": {
        "id": "nxIUC4LJcIUO"
      },
      "id": "nxIUC4LJcIUO"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic data based on the ideal logistic model\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define coefficients for the logistic model\n",
        "beta_0 = -1  # Intercept\n",
        "beta_1 = 2   # Slope\n",
        "\n",
        "# Generate the feature values (X)\n",
        "X = np.linspace(-3, 3, 10000).reshape(-1, 1)\n",
        "\n",
        "# Calculate probabilities using the logistic function\n",
        "logit = beta_0 + beta_1 * X.flatten()\n",
        "probabilities = 1 / (1 + np.exp(-logit))\n",
        "\n",
        "# Sample 0-1 values based on the probabilities\n",
        "y = np.random.binomial(1, probabilities)\n",
        "\n",
        "# Fit Logistic Regression\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X, y)\n",
        "logistic_preds = logistic_model.predict_proba(X)[:, 1]  # Probabilities for class 1\n",
        "predicted_classes_logistic = logistic_model.predict(X)  # Predicted classes (0 or 1)\n",
        "\n",
        "# Fit Linear Regression\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X, y)\n",
        "linear_preds = linear_model.predict(X)\n",
        "\n",
        "# Plot predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='black', label='True Labels', alpha=0.6)\n",
        "plt.plot(X, logistic_preds, color='blue', label='Logistic Regression (Sigmoid Curve)')\n",
        "plt.plot(X, linear_preds, color='red', label='Linear Regression')\n",
        "\n",
        "# Highlight inappropriate probabilities\n",
        "plt.axhline(0, color='gray', linestyle='--', alpha=0.7)\n",
        "plt.axhline(1, color='gray', linestyle='--', alpha=0.7)\n",
        "plt.axvline(0.5, color='gray', linestyle='--', alpha=0.7)\n",
        "plt.fill_between(X.flatten(), linear_preds.flatten(), 1, where=(linear_preds > 1), color='red', alpha=0.3, label='Invalid (>1, <0)')\n",
        "plt.fill_between(X.flatten(), linear_preds.flatten(), 0, where=(linear_preds < 0), color='red', alpha=0.3, label=None)\n",
        "\n",
        "# Add classification threshold information\n",
        "threshold = 0.5  # Default threshold for logistic regression\n",
        "plt.axhline(threshold, color='blue', linestyle='--', label='Classification Threshold (0.5)')\n",
        "plt.scatter(X, predicted_classes_logistic, color='cyan', label='Predicted Classes (Logistic)', alpha=0.5, marker='x')\n",
        "\n",
        "plt.title(\"Comparison of Logistic and Linear Regression Predictions\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Prediction / Probability\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6eFK0pfocbA9"
      },
      "id": "6eFK0pfocbA9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What about the intersection point?\n",
        "\n",
        "The blue curve (logistic regression sigmoid) and the red curve (linear regression line) do not necessarily intersect with the blue dashed line (classification threshold) at the same point due to their fundamentally different modeling approaches. Furthermore, if logistic regression were merely a sigmoid transformation of the linear regression model, the linear regression line would rather cross 0.0 exactly where the sigmoid crosses 0.5, which is also **not the case**.\n",
        "\n",
        "### Logistic Regression:\n",
        "\n",
        "- Models the log-odds (logarithm of the odds of the event happening) as a linear combination of features:  \n",
        "$$\n",
        "\\text{logit}(p) = \\beta_0 + \\beta_1 X\n",
        "$$\n",
        "\n",
        "- Maps the log-odds to a probability using the sigmoid function:  \n",
        "$$\n",
        "p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n",
        "$$\n",
        "\n",
        "- The sigmoid function introduces non-linearity, creating the characteristic S-shape. The intersection with the threshold (e.g., $p = 0.5$) occurs at the point where the log-odds equals zero:  \n",
        "$$\n",
        "\\beta_0 + \\beta_1 X = 0\n",
        "$$\n",
        "\n",
        "### Linear Regression:\n",
        "\n",
        "- Models the target variable directly as a linear combination of features:  \n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 X + \\epsilon\n",
        "$$\n",
        "\n",
        "- Does not account for probabilities or thresholds, and it extends predictions outside the valid range $[0, 1]$ for probabilities.\n",
        "\n",
        "- The red curve intersects the blue dashed line wherever the linear equation equals the threshold value (e.g., $y = 0.5$), which generally differs from where the logistic regression curve crosses $p = 0.5$.\n",
        "\n",
        "### Breaking symmetry\n",
        "\n",
        "**But if the distribution of\n",
        "$x$ is symmetric, and the data are generated in a balanced way (logistic center at $x=0.5$), shouldn’t the best linear approximation also cross 0.5 at $x=0.5$? Wouldn’t it break symmetry if it shifted left or right?**\n",
        "\n",
        "Well, the logistic function is centered (in terms of probability = 0.5) at $x=0.5$. But the data is sampled from $[−3,3]$ in a way that is symmetric about $x=0$. These two centers (0.5 vs. 0) do not coincide.\n"
      ],
      "metadata": {
        "id": "zl4ViXCXgaS_"
      },
      "id": "zl4ViXCXgaS_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteratively Reweighted Least Square (IRLS)\n",
        "\n"
      ],
      "metadata": {
        "id": "3pnwCtdcgxYI"
      },
      "id": "3pnwCtdcgxYI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Understanding IRLS**\n",
        "\n",
        "📚 **See also** → *The Elements of Statistical Learning*:  \n",
        "Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springer, 2009.  \n",
        "[📖 Link to the book](https://hastie.su.domains/ElemStatLearn/)\n",
        "(Chapter 4.4)\n",
        "\n",
        "\n",
        "The **IRLS algorithm** solves logistic regression by iteratively updating the coefficients $\\beta$. At each iteration, the algorithm performs a weighted least squares regression using the current estimate of probabilities."
      ],
      "metadata": {
        "id": "TA_b7dpnh1_x"
      },
      "id": "TA_b7dpnh1_x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **The Iterative Process**\n",
        "\n",
        "At iteration $t$, given current parameter estimate $\\beta^{(t)}$, the update to $\\beta^{(t+1)}$ is:\n",
        "\n",
        "$$\n",
        "\\beta^{(t+1)} = \\beta^{(t)} + \\left( X^\\top W^{(t)} X \\right)^{-1} X^\\top W^{(t)} \\left[ y - p^{(t)} \\right],\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "p^{(t)} = \\sigma\\left( X \\beta^{(t)} \\right)\n",
        "$$\n",
        "\n",
        "are the current predicted probabilities (using the logistic sigmoid),\n",
        "\n",
        "$$\n",
        "W^{(t)} = \\text{diag}\\left[ p^{(t)} \\odot \\left( 1 - p^{(t)} \\right) \\right]\n",
        "$$\n",
        "\n",
        "is the diagonal weight matrix, and $y$ is the vector of binary outcomes.\n",
        "\n",
        "Equivalently, one often sees the \"adjusted response\" form:\n",
        "\n",
        "$$\n",
        "\\beta^{(t+1)} = \\left( X^\\top W^{(t)} X \\right)^{-1} X^\\top W^{(t)} z^{(t)},\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "z^{(t)} = X \\beta^{(t)} + \\left( W^{(t)} \\right)^{-1} \\left( y - p^{(t)} \\right).\n",
        "$$\n"
      ],
      "metadata": {
        "id": "J4UBzIAnh9-P"
      },
      "id": "J4UBzIAnh9-P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Linear Regression as the First IRLS Iteration**\n",
        "\n",
        "A common story is:\n",
        "\n",
        "- Start IRLS by setting $\\beta^{(0)} = 0$ (or all parameters to zero).\n",
        "- Then set $p^{(0)} = 0.5$ for all samples (because $\\sigma(0) = 0.5$).\n",
        "- Hence $W^{(0)} = \\text{diag}(0.25, 0.25, \\dots)$.\n",
        "\n",
        "The claim you sometimes hear is that \"with constant weights, the first update is just OLS.\" However:\n",
        "\n",
        "\n",
        "Even with constant weights $W^{(0)} = 0.25I$, the working response $z^{(0)}$ is not simply $y$, but:\n",
        "\n",
        "$$\n",
        "z^{(0)} = X\\beta^{(0)} + \\left(W^{(0)}\\right)^{-1}(y - p^{(0)}) = 0 + 4(y - 0.5) = 4(y - 0.5).\n",
        "$$\n",
        "\n",
        "Therefore, the first update becomes:\n",
        "\n",
        "$$\n",
        "\\beta^{(1)} = \\left(X^\\top 0.25 X\\right)^{-1} X^\\top (0.25)[4(y - 0.5)],\n",
        "$$\n",
        "\n",
        "which simplifies to:\n",
        "\n",
        "$$\n",
        "\\beta^{(1)} = 4\\left(X^\\top X\\right)^{-1} X^\\top [(y - 0.5)].\n",
        "$$\n",
        "\n",
        "This is generally not the same as the ordinary least squares solution:\n",
        "\n",
        "$$\n",
        "\\beta_{\\text{OLS}} = \\left(X^\\top X\\right)^{-1} X^\\top y.\n",
        "$$\n",
        "\n",
        "In particular, notice two key differences compared to OLS:\n",
        "1. You have $(y - 0.5)$ in place of $y$.\n",
        "2. You have an extra scalar factor of $4$.\n",
        "\n"
      ],
      "metadata": {
        "id": "284G_RSGh6_W"
      },
      "id": "284G_RSGh6_W"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='black', label='True Labels', alpha=0.6)\n",
        "plt.plot(X, logistic_preds, color='blue', label='Logistic Regression (Sigmoid Curve)')\n",
        "\n",
        "# Highlight inappropriate probabilities\n",
        "plt.axhline(0, color='gray', linestyle='--', alpha=0.7)\n",
        "plt.axhline(1, color='gray', linestyle='--', alpha=0.7)\n",
        "plt.fill_between(X.flatten(), linear_preds.flatten(), 1, where=(linear_preds > 1), color='red', alpha=0.3, label='Invalid (>1, <0)')\n",
        "plt.fill_between(X.flatten(), linear_preds.flatten(), 0, where=(linear_preds < 0), color='red', alpha=0.3, label=None)\n",
        "\n",
        "# Add classification threshold information\n",
        "threshold = 0.5  # Default threshold for logistic regression\n",
        "plt.axhline(threshold, color='blue', linestyle='--', label='Classification Threshold (0.5)')\n",
        "plt.scatter(X, predicted_classes_logistic, color='cyan', label='Predicted Classes (Logistic)', alpha=0.5, marker='x')\n",
        "\n",
        "plt.plot(\n",
        "    X,\n",
        "    linear_preds,\n",
        "    color='red',\n",
        "    label=f'Linear Regression: slope={linear_model.coef_[0]:.4f}, int={linear_model.intercept_:.3f}'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# IRLS Implementation\n",
        "# -----------------------\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# 1) Prepare the initial guess: set all betas to 0\n",
        "params_current = np.array([0, 0])\n",
        "all_params = []\n",
        "\n",
        "# 2) Build a design matrix [1, x].\n",
        "X_design = np.hstack([np.ones((X.shape[0], 1)), X])  # shape (n, 2)\n",
        "\n",
        "# 3) IRLS loop until consecutive parameter update is < 1e-7\n",
        "while True:\n",
        "    # Current linear predictor\n",
        "    z_current = X_design @ params_current\n",
        "\n",
        "    # Current sigmoid\n",
        "    p = sigmoid(z_current)\n",
        "\n",
        "    # Diagonal weights: w_i = p_i * (1 - p_i)\n",
        "    # (Need to avoid zero weights in dividing, often you can clip p in practice,\n",
        "    #  but for clarity, we'll keep it simple.)\n",
        "    W = p * (1 - p)\n",
        "\n",
        "    # Adjusted response: z_i = z_current_i + (y_i - p_i) / w_i\n",
        "    # In code, use safe fraction for p*(1-p) if desired, here we assume no perfect 0 or 1.\n",
        "    z = z_current + (y - p) / W\n",
        "\n",
        "    # Weighted least squares update:\n",
        "    # new_params = (X^T W X)^(-1) X^T W z\n",
        "    W_mat = np.diag(W)               # shape (n, n)\n",
        "    XTW = X_design.T @ W_mat         # shape (2, n)\n",
        "    XTWX = XTW @ X_design            # shape (2, 2)\n",
        "    XTWz = XTW @ z                   # shape (2, )\n",
        "    new_params = np.linalg.inv(XTWX) @ XTWz\n",
        "\n",
        "    # Check convergence\n",
        "    diff = np.linalg.norm(new_params - params_current)\n",
        "    all_params.append(new_params)\n",
        "    print(f'Iteration {len(all_params)}: diff = {diff}')\n",
        "    params_current = new_params\n",
        "\n",
        "    if diff < 1e-7:\n",
        "        break\n",
        "\n",
        "print(f'Performed {len(all_params)} iterations.')\n",
        "# At this point, all_params[0] is iteration-1 (one after the 0-based start),\n",
        "# and all_params[-1] is the final IRLS solution.\n",
        "\n",
        "\n",
        "# 5) Show the straight lines in selected IRLS iterations\n",
        "\n",
        "selected_indices=[0,1,2,7]\n",
        "num_lines = len(selected_indices)\n",
        "\n",
        "for i_line, irls_idx in enumerate(selected_indices, start=1):\n",
        "    # fraction from 1..4 => color goes from near-red to blue\n",
        "    frac = i_line / num_lines  # 1/4 to 4/4\n",
        "    # simple linear interpolation from red=(1,0,0) to blue=(0,0,1)\n",
        "    color_i = (0, 1 - frac, frac)\n",
        "\n",
        "    p_ = all_params[irls_idx]\n",
        "    # Build a grid for plotting the straight line\n",
        "    x_plot = np.linspace(X.min(), X.max(), 200)\n",
        "    y_plot = p_[0] + p_[1] * x_plot\n",
        "\n",
        "    plt.plot(\n",
        "        x_plot,\n",
        "        y_plot,\n",
        "        color=color_i,\n",
        "        label=f'IRLS iteration {irls_idx + 1}: slope={p_[1]:.4f}, int={p[0]:.3f}'  # shows the real iteration number\n",
        "    )\n",
        "\n",
        "\n",
        "# 5) Add a very thin dotted black vertical line where the final IRLS line crosses 0.\n",
        "#    For y = b + w*x = 0 => x = -b/w, if w != 0.\n",
        "final_index = len(all_params) - 1\n",
        "final_params = all_params[final_index]\n",
        "\n",
        "b_final, w_final = final_params[0], final_params[1]\n",
        "if abs(w_final) > 1e-14:\n",
        "    x_decision = -b_final / w_final\n",
        "    plt.axvline(x_decision, color='black', linestyle=':', linewidth=0.5)\n",
        "\n",
        "# 6) Finally, keep y-limits from -0.2 to 1.2\n",
        "plt.ylim(-0.2, 1.2)\n",
        "\n",
        "# 7) Show the legend, so we can distinguish these lines\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pl930X8lg3dd"
      },
      "id": "pl930X8lg3dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Optimizing the Classification Threshold in Logistic Regression**\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "In this assignment, you will explore the concept of **classification thresholds** in **Logistic Regression** and investigate whether the commonly used threshold of **0.5** is always the optimal choice. To complete this assignment, you will need to conduct **additional reading** on **ROC curves** and the **AUC metric**, beyond the materials covered in class.\n",
        "\n",
        "This exercise focuses on the **importance of threshold selection** and how it impacts model performance based on different evaluation criteria.\n",
        "\n",
        "\n",
        "\n",
        "## **The Objective**\n",
        "\n",
        "**Is a 0.5 threshold always the best choice in Logistic Regression?**\n",
        "\n",
        "- Analyze if the standard threshold of **0.5** always yields the most desirable results in various scenarios.\n",
        "- Investigate alternative thresholds and how they can improve model performance depending on the problem context.\n",
        "\n",
        "\n",
        "\n",
        "## **Helper Questions**\n",
        "\n",
        "1. **Would a different threshold yield better results?**  \n",
        "   - How does shifting the threshold affect the **trade-off between sensitivity and specificity**?\n",
        "\n",
        "2. **How do you define \"better\" results?**  \n",
        "   - Is **higher specificity** more valuable than **higher sensitivity** in certain contexts? Or maybe the other way around?\n",
        "   - Does the **best threshold** depend on the task? *(discuss cases like disease detection vs. spam filtering)*\n",
        "\n",
        "3. **How does a ROC curve help in this process?**  \n",
        "   - Learn how to use a **ROC curve** to visualize the trade-off between **True Positive Rate (Sensitivity)** and **False Positive Rate (1 - Specificity)**.\n",
        "\n",
        "4. **How do you select the optimal threshold using the ROC curve?**  \n",
        "   - Discuss strategies to select a threshold depending on the specific task at hand and the acceptable **trade-off between sensitivity and specificity**.\n",
        "\n",
        "5. **What is the AUC metric and how is it useful?**  \n",
        "   - Define **AUC (Area Under the Curve)** and discuss its role in evaluating the overall performance of a classifier.\n",
        "\n",
        "\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "1. **Colab Notebook**\n",
        "   - Select a suitable classification 2-class dataset.\n",
        "   - Divide the dataset into three parts: **train**, **validation**, and **test**.\n",
        "   - Train the **Logistic Regression** model on the **training** set.\n",
        "   - Use the **validation** set to:\n",
        "     - Plot the **ROC curve**.\n",
        "     - Calculate the **AUC**.\n",
        "     - Determine the **optimal threshold** using the ROC curve for your dataset.\n",
        "   - Apply the selected threshold to the **test** set to evaluate final model performance using metrics like:\n",
        "     - **Accuracy**\n",
        "     - **Precision & Recall**\n",
        "     - **F1-Score**\n",
        "     - **Specificity & Sensitivity**\n",
        "\n",
        "2. **Answer the Questions:**\n",
        "   - Use markdown cells to provide explanations.\n",
        "   - Answer all helper questions with supporting evidence from your plots and results.\n",
        "\n",
        "3. **Visualizations:**\n",
        "   - **ROC curve** based on the **validation** set with threshold points marked.\n",
        "   - Highlight the **optimal threshold** on the ROC curve.\n",
        "\n",
        "4. **Publish on GitHub**  \n",
        "   - Place the Colab notebook in your **GitHub repository** for this course.\n",
        "   - In your repository’s **README**, add a **link** to the notebook and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B1K2cKMGKi0Z"
      },
      "id": "B1K2cKMGKi0Z"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}